# **Behavioral Cloning**
## Project 3

---

**Behavioral Cloning Project**
The workflow for this project included:

1. Loading the appropriate data.
2. Implementing various augmentation techniques in order to help generalize the model. These techniques were based off of prior research and experience with improving the accuracy of deep learning models. Additionally, along the way visualizations of random images were checked to make sure they were being augmented appropriately.
3. Training and validation datasets were created. In this case generators were used in order to pull data on the fly and make the process more memory efficient.
4. Creating a convolutional neural network that suits the problem. In this case, the comma.ai steering model was used due to its simplicity and applicability to the problem.
5. Assigning hyperparameters and training the model on the training and validation datasets.
6. Testing that the model successfully navigates around Track 1 and possibly Track 2.

Ultimately, the goal of this project was to train a machine learning model to drive like a human. The approach I took began with using the example images and proving the model before collecting any data. Once the model was constructed and was able to pilot the car autonomously around the track, additional data was collected and used for the final model.

[//]: # (Image References)

[image1]: ./Report_Figures/model.png "Model Visualization"
[image2]: ./Report_Figures/model_summary.png "Model Summary"
[image3]: ./Report_Figures/center_2017_05_05_23_18_59_360.jpg "Center camera"
[image4]: ./Report_Figures/center_2017_05_08_07_33_41_810.jpg "Recovery Sequence 1"
[image5]: ./Report_Figures/center_2017_05_08_07_33_42_167.jpg "Recovery Sequence 2"
[image6]: ./Report_Figures/center_2017_05_08_07_33_42_768.jpg "Recovery Sequence 3"
[image7]: ./Report_Figures/Augmentation_pipeline.png "Image Augmentation Pipeline"

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode
My project includes the following files:
- model.py: Contains the script to create and train the model. This file was modified extensively to create the model.
- drive.py: For driving the car in autonomous mode. This file was modified to accept data from model.py.
- model_test.h5: Contains a trained convolutional neural network.
- writeup_20170509.md: Summarizes the results and methods.

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing:

```sh
python drive.py model_test.h5
```

#### 3. Submission code is usable and readable
The model.py file contains the code for training and saving the convolutional neural network. The file shows the pipeline I used for training and validating the model and it contains comments to explain how the code works. In addition, drive.py was slightly modified to accomodate the incoming data and to fix the throttle speed at 0.1.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed
My model is based on the [comma.ai](https://github.com/commaai/research/blob/master/train_steering_model.py) self-driving steering model. The comma.ai model is relatively small in size and not overly complicated. In addition, it's a proven model that's being used in production. The neural network is built using:

- A normalization layer to normalize all input images (Lambda used for normalization)
- Convolutional layer, 16 8x8 feature maps, ELU activation function
- Convolutional layer, 32 8x8 feature maps, ELU activation function
- Convolutional layer, 64 8x8 feature maps, ELU activation function
- Flatten layer
- 20% Dropout
- ELU non-linearity layer
- Fully connected layer with 512 units, ELU activation function
- 50% Dropout
- ELU non-linearity layer
- Fully connected output layer

This model produces satisfactory results without being processor or GPU intensive.

#### 2. Attempts to reduce overfitting in the model
The model reduces overfitting by introducing dropout at two different places.

The model was initially trained and validated on the dataset provided by Udacity. Then, after the appropriate image augmentation and model had been implemented, data was collected using the simulator and the model trained on that.

#### 3. Model parameter tuning
The model used the Adam optimizer. All default parameters were used except for the learning rate which was changed to 1e-4. I saw that larger learning rates caused the vehicle to drive less smoothly than a human and make more abrupt steering decisions.

#### 4. Appropriate training data
As mentioned above, the model was initially trained and validated on the dataset provided by Udacity. Then, after the appropriate image augmentation and model had been implemented, data was collected using the simulator and the model trained on that. The data was captured using three cameras, left, center, and right.

#### 5. Solution Design Approach
The goal of this design was to create a model that would learn human driving behavior based on images collected in a simulator.

Images collected in the simulator were first preprocessed and augmented in order to help the model generalize better. Most of the image augmentations functions used in the traffic sign classifier project were used for this project as well. In most cases, the only thing that needed to be changed was the addition of the steering angle as an argument. The augmentations were: horizontal flipping, brightness adjustment, and x and y translation. In addition, the lower area of each image was cropped to remove the hood of the car. Image augmentation techniques were inspired by [Vivek Yadav](https://github.com/vxy10/ImageAugmentation) and [Alex Staravoitau](http://navoshta.com/end-to-end-deep-learning/).

As mentioned in the classroom, a generator was used to reduce the memory needed to run this model. preprocess_image_train() takes each line from a given .csv file, randomly chooses a left, right, or center image, then applies the image augmentations described above and returns the augmented image and steering angle. generate_batch_train() generates data randomly to be used for training while generate_batch_valid() generates data used for validation. Since most of the training data consists of images with small steering angles (~0.0) a method was needed to decrease the probability of small steering angles being used in training the model. A [callback](https://keras.io/callbacks/) in Keras was used to accomplish this and [this](https://keunwoochoi.wordpress.com/2016/07/16/keras-callbacks/) blog post was helpful in explaining what each part of the class does. The threshold variable is reduced after each epoch so that more small steering angle images (i.e. straight line driving) is used in training.

The [comma.ai](https://github.com/commaai/research/blob/master/train_steering_model.py) self-driving steering model was used to predict steering angles. The Adam optimizer was used with a learning rate of 1e-4 and a mean squared error loss function.

At the end of the process, the vehicle was able to drive around Track 1 without any problems.

#### 6. Final Model Architecture
The final model architecture is visualized below using the plot() function in Keras.

![comma.ai][image1]

The model summary output from Keras is below.

![Model summary][image2]

The network has 592,497 parameters. This is a relatively modest amount, however, driving performance suggests that it is sufficient.

#### 7. Creation of the Training Set & Training Process

The main goal of capturing training data was to provide good driving behavior. To accomplish this, a total of three laps around Track 1 were completed. The image below shows the view from the center camera while the vehicle is driving straight.

![Center camera][image3]

During the three laps around the track, the vehicle was steered towards the edge of the track and then brought back to the center. This provided training for instances when the vehicle moved away from the center of the track and needed to recover.

![Left side recovery 1][image4]
![Left side recovery 2][image5]
![Left side recovery 3][image6]

After the collection process, there were a total of 27,264 images. 9,088 images from each of the left, right, and center cameras. I found that collecting additional data on Track 2 actually decreased my training and validation accuracy so for this project I used only data collected on Track 1.

I used an image augmentation pipeline to augment each image which can be seen in the figure below.

![Image augmentation][image7]

During the generation (generate_batch_train) of batched training sets the images are randomly flipped horizontally and the steering angle is reversed to account for the flip (preprocess_image_train). Next, images are translated in the x and y directions and a new steering angle is calculated based on the translation. Then, the brightness is shifted. In this case, the brightness is decreased so the image is slightly darker. Finally, the top 1/4 and bottom 25 pixels of the image are removed (everything above the horizon and the hood of the vehicle) and the image is resized to 64 x 64 pixels.

The network was trained on the training set for 50 epochs using 20,224 augmented samples per epoch. Based on the training and validation loss, approximately 20 epochs would have been sufficient. On my 2015 MacBook Pro, each epoch took approximately 2 minutes to run.

The final video is below.

![Final Video](./Track_1_Submission.mp4)
