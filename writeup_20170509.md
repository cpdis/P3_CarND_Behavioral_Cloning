https://github.com/vxy10/ImageAugmentation
# **Behavioral Cloning**

## Project 3

---

**Behavioral Cloning Project**

The goals/steps of this project are the following:
- Use the simulator to collect data of good driving behavior
- Build, a convolutional neural network in Keras that predicts steering angles from images
- Train and validate the model with a training and validation set
- Test that the model successfully drives around track one without leaving the road
- Summarize the results with a written report

Ultimately, the goal of this project was to train a machine learning model to drive like a human. The approach I took began with using the example images and proving the model before collecting any data. Once the model was constructed and was able to pilot the car autonomously around the track, then data was collected and used for the final model.

[//]: # (Image References)

[image1]: ./examples/placeholder.png "Model Visualization"
[image2]: ./examples/placeholder.png "Grayscaling"
[image3]: ./examples/placeholder_small.png "Recovery Image"
[image4]: ./examples/placeholder_small.png "Recovery Image"
[image5]: ./examples/placeholder_small.png "Recovery Image"
[image6]: ./examples/placeholder_small.png "Normal Image"
[image7]: ./examples/placeholder_small.png "Flipped Image"

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
- model.py: Contains the script to create and train the model. This file was modified extensively to create the model.
- drive.py: For driving the car in autonomous mode. This file was modified to accept data from model.py.
- model_test.h5: Containins a trained convolution neural network.
- writeup_report.md or writeup_report.pdf summarizing the results

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing:

```sh
python drive.py model_test.h5
```

#### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolutional neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works. In addition, drive.py was slightly modified to accomodate the incoming data and to fix the throttle speed at 0.1.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

My model is based on the [comma.ai](https://github.com/commaai/research/blob/master/train_steering_model.py) self-driving steering model. The comma.ai model is relatively small in size and not overly complicated. In addition, it's a proven model that's being used in production. The neural network is built using:

- A normalization layer to normalize all input images (Lambda used for normalization)
- Convolutional layer, 16 8x8 feature maps, ELU activation function
- Convolutional layer, 32 8x8 feature maps, ELU activation function
- Convolutional layer, 64 8x8 feature maps, ELU activation function
- Flatten layer
- 20% Dropout
- ELU non-linearity layer
- Fully connected layer with 512 units, ELU activation function
- 50% Dropout
- ELU non-linearity layer
- Fully connected output layer

This model produces satisfactory results without being processor or GPU intensive.

#### 2. Attempts to reduce overfitting in the model

The model reduces overfitting by introducing dropout at two different places.

The model was initially trained and validated on the dataset provided by Udacity. Then, after the appropriate image augmentation and model had been implemented, data was collected using the simulator and the model trained on that.

#### 3. Model parameter tuning

The model used the Adam optimizer. All default parameters were used except for the learning rate which was changed to 1e-4. I saw that larger learning rates caused the vehicle to drive less smoothly than a human and make more abrupt steering decisions.

#### 4. Appropriate training data

As mentioned above, the model was initially trained and validated on the dataset provided by Udacity. Then, after the appropriate image augmentation and model had been implemented, data was collected using the simulator and the model trained on that. The data was captured using three cameras, left, center, and right.

#### 5. Solution Design Approach

The goal of this design was to create a model that would learn human driving behavior based on images collected in a simulator.

Images collected in the simulator were first preprocessed and augmented in order to help the model generalize better. Most of the image augmentations functions used in the traffic sign classifier project were used for this project as well. In most cases, the only thing that needed to be changed was the addition of the steering angle as an argument. The augmentations were: horizontal flipping, brightness adjustment, and x and y translation. In addition, the lower area of each image was cropped to remove the hood of the car and each image was resized to 64 x 64 pixels.

As mentioned in the classroom, a generator was used to reduce the memory needed to run this model. preprocess_image_train() takes each line from a given .csv file, randomly chooses a left, right, or center image, then applies the image augmentations described above and returns the augmented image and steering angle. generate_batch_train() generates data randomly to be used for training while generate_batch_valid() generates data used for validation.



At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.

#### 6. Final Model Architecture

The final model architecture (model.py lines 18-24) consisted of a convolution neural network with the following layers and layer sizes ...

Here is a visualization of the architecture (note: visualizing the architecture is optional according to the project rubric)

![alt text][image1]

#### 7. Creation of the Training Set & Training Process

To capture good driving behavior, I first recorded two laps on track one using center lane driving. Here is an example image of center lane driving:

![alt text][image2]

I then recorded the vehicle recovering from the left side and right sides of the road back to center so that the vehicle would learn to .... These images show what a recovery looks like starting from ... :

![alt text][image3]
![alt text][image4]
![alt text][image5]

Then I repeated this process on track two in order to get more data points.

To augment the data sat, I also flipped images and angles thinking that this would ... For example, here is an image that has then been flipped:

![alt text][image6]
![alt text][image7]

Etc ....

After the collection process, I had X number of data points. I then preprocessed this data by ...


I finally randomly shuffled the data set and put Y% of the data into a validation set.

I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was Z as evidenced by ... I used an adam optimizer so that manually training the learning rate wasn't necessary.
